{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes from \n",
    "\n",
    "* Tensorflow for Deep Learning by Bharath Ramsundar and Reza Bosagh Zadeh (O'Reilly). Copyright 2018 Reza Zadeh, Bharath Ramsundar, 978-1-491-98045-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Primitives\n",
    "\n",
    "Most deep architectures are built by combining and recombining a limited set of architectural primitives.\n",
    "\n",
    "### Fully Connected Layer\n",
    "\n",
    "* Transforms a list of inputs into a list of outputs  \n",
    "* Any input value can affect any output value  \n",
    "* Have many learnable parameters\n",
    "* Large advantage of assuming no structure in the inputs\n",
    "\n",
    "### Convolutional Layer (images to images)\n",
    "\n",
    "* A convolutional network assumes special spatial structure in its input\n",
    "* In particular, it assumes that inputs that are close to each other spatially are semantically relates  \n",
    "* Makes most sense for images  \n",
    "* Convolutional layers transform images into images\n",
    "\n",
    "### Recurrent Neural Network Layers (RNN)\n",
    "\n",
    "* Allow neural networks to learn from sequences of inputs. \n",
    "* Assumes that the input evolves from step to step following a defined update rule that can be learned from data\n",
    "* This update rule presents a prediction of the next state in the sequence given all the states that have come previously\n",
    "* Very useful for tasks such as language modeling, where engineers seek to build systems that can predict the next word users will type from history\n",
    "\n",
    "### Long Short-Term Memory Cells\n",
    "\n",
    "* Is a modification to the RNN layer that allows for signals from deeper in the past to make their way to the present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Architectures\n",
    "\n",
    "Nnot an exhaustive list\n",
    "\n",
    "### LeNet\n",
    "\n",
    "* Arguably the first prominent \"deep\" convolutional architecture\n",
    "* Introduced in 1988\n",
    "* Performed optical character recognition (OCR) for documents\n",
    "* Computational cost of the LeNet was estreme for computer hardwares available at the time\n",
    "\n",
    "### AlexNet\n",
    "\n",
    "* Based on a modification of LeNet run on powerful graphical processing units (GPUs)\n",
    "* In 2010 The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) was first organized\n",
    "* In 2012, the AlexNet architecture, entered and dominated the challenge with error rates half that of the nearest competitors\n",
    "\n",
    "\n",
    "### ResNet\n",
    "\n",
    "* Winner of the ILSVRC 2015 challenge\n",
    "* Extended up to 130 layers deep, in contrast to 8-layer AlexNet architecture\n",
    "* When networks grow this big, they run into the vanishing gradients problem\n",
    "* The ResNet introduced an innovation that controlled this attenuation\n",
    "* Allows part of the signal from deeper layers to pass through undiminished\n",
    "\n",
    "### Neural Captioning Model\n",
    "\n",
    "* Automatically generate captions for the contents of images\n",
    "* They do so by combining convolutional networks with an LSTM layer\n",
    "* The entire system is trained *end-to-end*\n",
    "\n",
    "### Google Neural Machine Translation\n",
    "\n",
    "* Uses the paradigm of end-to-end training\n",
    "* depends on the fundamental building block of the LSTM, which it stacks over a dozen times and trains on an extremely large dataset of translated sentences\n",
    "* breakthrough advance in machine-translation by cutting the gap between human and machine translations by up to 60%\n",
    "\n",
    "### One-Shot Models\n",
    "\n",
    "* Perhaps the most interesting new idea in machine/deep learning\n",
    "* Given only a few examples, such systems can learn to make meaningful predictions with very few datapoints\n",
    "\n",
    "### AlphaGo\n",
    "\n",
    "* AlphaGo from Google DeepMind deafeated one of the world's strongest Go champions\n",
    "* Some of the key ideas from AlphaGo include the use of a deep value network and deep policy network\n",
    "* The value network provides an estimate of the value of a board position\n",
    "* The policy network helps estimate the best move to take given a current board state\n",
    "* Monte Carlo Tree search, together with the above two techniques overcame the large branching factor in Go games\n",
    "\n",
    "### Generative Adversarial Networks (GANs)\n",
    "\n",
    "* Uses two competing neural networks, the generator and the adversary (also called the discriminator)\n",
    "* The generator tries to draw samples from a training distribution (tries to draw realistic images)\n",
    "* The discriminator works on differentiating samples drawn from the generator from true data samples\n",
    "\n",
    "### Neural Turing Machines\n",
    "\n",
    "* First attempt at making a deep learning architecture capable of learning arbitrary algorithms\n",
    "* Adds an external memory bank to an LSTM-like system\n",
    "* Allows the deep architecture to make use of scratch space to compute more sophisticated functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
