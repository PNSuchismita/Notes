{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Models for Time Series and Natural Language Processing\n",
    "\n",
    "Course 4 of 5 in the [Advanced Machine Learning with TensorFlow on Google Cloud Platform Specialization](https://www.coursera.org/specializations/advanced-machine-learning-tensorflow-gcp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Working with Sequencies](#working_with_sequencies)\n",
    "3. [Recurrent Neural Networks (RNNs)](#rnn)\n",
    "4. [Dealing with Longer Sequencies (LSTMs)](#lstm)\n",
    "5. [Reusable Embeddings](#reusable_empeddings)\n",
    "6. [Encoder-Decoder Models](#encoder_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "<a id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Working with Sequencies\n",
    "\n",
    "<a id=\"working_with_sequencies\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What a sequence is?**  \n",
    "\n",
    "Sequences are data points that can be meaningfully ordered such that earlier observations provide information about later observations.\n",
    "* Identifying sequence data\n",
    "    * Flip of the coin during a period of time _ not a sequence data\n",
    "    * Bend the coin sightly depending on the outcome so that eventually the coin flip will always result in heads _ sequence data\n",
    "    * Natural Language _ sequence data\n",
    "    * Images _ sometimes\n",
    "    * Movies _ sequence data \n",
    "* Broadly, sequence models fit into 3 types.\n",
    "    * one-to-sequence: a non-sequence is passed in and the model yields a sequence.\n",
    "         * Use CNN to extract features and then use one to sequence to produce the output.\n",
    "    * sequence-to-one: a sequence is passed in and the model yields a non-sequence.\n",
    "         * SpartReply, uses a sequence as an input to produce one of the predefined outputs as a response. *Note*: Just because the output looks like a sequence doesn't mean that it needs to be.\n",
    "    * sequence-to-sequence: a sequence is passed in and the model yields a sequence.\n",
    "         * translation\n",
    "\n",
    "### How to prepare a sequence data for modeling?  \n",
    "\n",
    "* The simplest method is to take a window of fixed size and slide it over our dataset, and then to concatenate the observations within the window to create a vector.\n",
    "* Assuming we have observations at $t$ different time points, and our sliding window is $n$ time units wide, and we have a stride of 1, how many rows will there be in our training set?\n",
    "    * Answer: $t-n$\n",
    "\n",
    "### Classical approached to sequence modeling.\n",
    "    \n",
    "Once you've decided you're sliding window size and concatenated the data in the window, the next step is putting it into your model. \n",
    "\n",
    "#### Modeling Sequencies with Linear Models\n",
    "\n",
    "* Imagine that you want to predict the height of water being sprayed from an oscillating sprinkler. Oscillating sprinklers consist of a cylindrical metal tube that rotates about its small axis and sprays water through holes drilled down the length of the tube.  \n",
    "* In out synthetic data set, instead of a fixed rotation speed and a fixed water pressure, we will actually randomly generate both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_time_series():\n",
    "    freq = (np.random.random()*0.5) + 0.1                       # 0.1 to 0.6\n",
    "    ampl = np.random.random() + 0.5                             # 0.5 to 1.5\n",
    "    noise = [(np.random.random()*0.3) for i in range(SEQ_LEN)]   # -0.3 to 0.3 uniformly distributed\n",
    "    x = np.sin(np.arrange(0, SEQ_LEN)*freq)*ampl + noise\n",
    "    return x\n",
    "\n",
    "# write data to disc\n",
    "def to_csv(filename, N):\n",
    "    with open(filename, 'w') as ofp: \n",
    "        for lineno in xrange(0, N):\n",
    "            seq = create_time_series()\n",
    "            line = \",\".join(map(str, seq))\n",
    "            ofp.write(line + \"\\n\")\n",
    "            \n",
    "# creating an input function\n",
    "def read_dataset(filename, mode, batch_size=512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(row):\n",
    "            # row is a string tensor containing the contents of one row\n",
    "            features = tf.decode_csv(row, record_defaults=DEFAULTS)\n",
    "    # string tensor -> list of 50 rank 0 float tensors\n",
    "            label = features.pop()           # remove last feature and use as label\n",
    "            features = tf.stack(features)    # list of rank 0 tensors -> single rank 1 tensor\n",
    "            return {TIMESERIES_COL: features}, label\n",
    "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "        dataset = tf.data.Dataset.list_files(filename)\n",
    "        # Read in data from files\n",
    "        dataset = dataset.flat_map(tf.data.TextLineDataset)\n",
    "        # Parse text lines as comma-separated values (CSV)\n",
    "        dataset = dataset.map(decode_csv)\n",
    "        ...\n",
    "    return _input_fn\n",
    "\n",
    "# Compare model performance\n",
    "#eval_metric_ops = {\n",
    "#    \"RMSE\": rmse,\n",
    "#    \"RMSE_same_as_last\": same_as_last_benchmark(features, labels),\n",
    "#}\n",
    "\n",
    "# RMSE when predicting same as last value\n",
    "def same_as_last_benchmark(features, labels):\n",
    "    predictions = features[TIMESERIES_COL][:, -1]   # last value in input sequence\n",
    "    return tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "\n",
    "# Defining a Linear Model for Time Series Prediction\n",
    "def linear_model(features, mode, params):\n",
    "    X = features[TIMESERIES_COL]\n",
    "    predictions = tf.layers.dense(X, 1, activation=None)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recurrent Neural Networks (RNNs)\n",
    "\n",
    "<a id=\"rnn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dealing with Longer Sequencies (LSTMs)\n",
    "\n",
    "<a id=\"lstm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reusable Embeddings\n",
    "\n",
    "<a id=\"reusable_empeddings\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encoder-Decoder Models\n",
    "\n",
    "<a id=\"encoder_decoder\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
