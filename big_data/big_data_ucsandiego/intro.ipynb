{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Big Data\n",
    "\n",
    "[Big Data Scpesialization, UC San Diego, Coursera](https://www.coursera.org/specializations/big-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Need for Big Data\n",
    "\n",
    "A new **torrent of big data** combined with **computing capability anytime, anywhere** has been at the core of the launch of the big data era."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where does data come from?\n",
    "\n",
    "### 1. Machines\n",
    "\n",
    "**Largest source** of big data. Machines collect data 24/7 via their built-in sensors, both at personal and industrial scales. Ex, activity tracker.\n",
    "\n",
    "### 2. People\n",
    "\n",
    "Most of it is text-heavy and **unstructured** data. Ex, text, images, video, audio, etc. Open source big data frameworks that tackle the challenges of unstructured data.\n",
    "\n",
    "* **Hadoop** is designed to support the processing of large data sets in a distributed computing environment.\n",
    "* **Storm** and **Spark** are two other open source frameworks that handle real time data generated at a fast rate.\n",
    "\n",
    "Large unstructured datasets get stored in NoSQL databases in the cloud. A couple of exmples are\n",
    "\n",
    "* **Neo4j** is an example of a graph database.\n",
    "* **Cassandra** is an example of a key value database.\n",
    "\n",
    "### 3. Organizations\n",
    "\n",
    "* Structured but often Siloed.   \n",
    "* Usually, structured data is stored in relational database management systems.\n",
    "* However, we call any data that is in the form of a record located in a fixed field or file structured data. This definition also includes spreadsheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Key: Data Integration\n",
    "\n",
    "Data integration process\n",
    "\n",
    "* Discovering\n",
    "* Accessing\n",
    "* Monitoring\n",
    "* Modeling\n",
    "* Transforming\n",
    "\n",
    "Integration of diverse datasets significantly **reduces the overall data complexity** in the data-driven product.  \n",
    "The data becomes **more available** for use and **unified** as a system of its own.  \n",
    "Such a streamlined and integrated data system can **increase the collaboration** between different parts of your data systems.  \n",
    "**Add value** to your big data and improve your business even before you start analyzing it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characteristics of Big Data\n",
    "\n",
    "### **V**olume\n",
    "\n",
    "Volume is the big data dimension that relates to the sheer size of big data, which is growing exponentially.\n",
    "\n",
    "### **V**ariety\n",
    "\n",
    "* Variety == Complexity  \n",
    "* Axes of Data Variety\n",
    "    * Structural Variety - formats and models\n",
    "    * Media Variety - medium in which data get delivered\n",
    "    * Semantic Variety - how to interpret and operate on data\n",
    "    * Availability Variations - real-time? intermittent?\n",
    "    \n",
    "### **V**elocity\n",
    "\n",
    "* Velocity == Speed\n",
    "    * Speed of creating data\n",
    "    * Speed of storing data\n",
    "    * Speed of analyzing daata\n",
    "* Real-time processing\n",
    "    * Instantly capture streaming data -> Feed real time to machines -> Process real time -> Act\n",
    "    \n",
    "### **V**eracity\n",
    "\n",
    "* Veracity == Quality\n",
    "    * Validity\n",
    "    * Volatility\n",
    "* Accuracy of the data, reliability of the data source, context within analysis \n",
    "\n",
    "\n",
    "### **V**alence\n",
    "\n",
    "* Valence == Connectedness\n",
    "\n",
    "### **V**alue\n",
    "\n",
    "At the heart of the big data challenge is turning all of the other dimensions into truly useful business value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Five P's of Data Science\n",
    "\n",
    "* **People**: Data Science Teams and stakeholders\n",
    "* **Purpose**: Set of challenges defined by your big data strategy\n",
    "* **Process**: Defines the set of steps and how everyone can contribute to it. (**Aquire -> Prepare -> Analize -> Report -> Act**)\n",
    "* **Platforms**: Hadoop framework, or other computing platforms to scale different steps.\n",
    "* **Programmability**: In addition, the scalable process should be programmable through utilization of reusable and reproducible programming interfaces to libraries, like systems middleware, analytical tools, visualization environments, and end user reporting environments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Process of Data Science\n",
    "\n",
    "Aquire -> Prepare -> Analize -> Report -> Act\n",
    "\n",
    "### 1. Aquire\n",
    "\n",
    "* Identify suitable data\n",
    "* Aquire all available data\n",
    "\n",
    "\n",
    "| Type of Data | Type of the Tool to Access Data | Example |\n",
    "| ------------ | ----------------------------------- | ------------ |\n",
    "| Structural (Traditional Databases) | SQL and query browsers | Oracle SQL Developer, PostgreSQL | \n",
    "| Text Files | Scripting Languages | Python, Javascript, PhP, etc. |\n",
    "| Remote Data | Web Services | XML, REST, WebSocket, etc. |\n",
    "| NoSQL storage | API, WebServices | Casandra, mongoDB, HBASE, etc. |\n",
    "\n",
    "\n",
    "### 2. Prepare\n",
    "\n",
    "#### A. Exploring Data\n",
    "\n",
    "* Understand your data (general trends, correlations, outiers, etc.)\n",
    "* Describe your data (Mean, median, mode, range, standard deviation, etc.)\n",
    "* Visualize your data (Histograms, Heat maps, Boxplots, Line graphs, Scatterplots, etc.)\n",
    "\n",
    "#### B. Pre-Processing Data\n",
    "\n",
    "* **Clean** Inconsistent data, duplicate records, missing values, invalid data, outliers  \n",
    "* **Transform**: Scaling (chenging the range of the values), feature selection, dimentionality reduction, data manipulation (ex. grouping), transformation (to reduce noise and variability)\n",
    "\n",
    "### 3. Analize\n",
    "\n",
    "**Select Technique -> Build Model -> Validate Model**\n",
    "\n",
    "Categories of Analysis Techniques \n",
    "\n",
    "* Classification\n",
    "* Regression\n",
    "* Clustering\n",
    "* Graph Analytics\n",
    "* Association Analysis\n",
    "\n",
    "### 4. Report\n",
    "\n",
    "Communicating results, even the inconclusive results.\n",
    "\n",
    "\n",
    "### 5. Act\n",
    "\n",
    "Turn insights into action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Scalable Computing Concepts \n",
    "\n",
    "### Distributed File System\n",
    "\n",
    "* How the operating system manages files is called a file system.\n",
    "* When many storage computers are connected through the network, we call it a distributed file system. \n",
    "* Distributed file systems provide data scalability, fault tolerance, and high concurrency through partitioning and replication of data on many nodes.\n",
    "\n",
    "### Scalable Computing Over the Internet\n",
    "\n",
    "**Commodity clusters** are affordable parallel computers with an average number of computing nodes.\n",
    "* Not as powerful as traditional parallel computers.\n",
    "* Are often built out of less specialized nodes.\n",
    "* The nodes in the commodity cluster are more generic in their computing capabilities.\n",
    "* Computing nodes are clustered in racks connected to each other via a fast internet.\n",
    "* There might be many of such racks in extensible amounts.\n",
    "\n",
    "These type of systems have a higher potential for partial failures. \n",
    "* A node, or an entire rack can fail at any given time.\n",
    "* The connectivity of a rack to the network can stop.\n",
    "* The connections between individual nodes can break.\n",
    "\n",
    "**Fault-tolerance** is the ability to recover from such failures. For Fault-tolerance of such systems, two neat solutions emerged.\n",
    "* Redundant data storage\n",
    "* Restart of failed individual parallel jobs\n",
    "\n",
    "### Programming Model for Big Data\n",
    "\n",
    "* A programming model is an abstraction or existing machinery or infrastructure.\n",
    "* It is a set of abstract runtime libraries and programming languages that form a model of computation.\n",
    "\n",
    "Requirements for Big Data Programming Model\n",
    "1. Should support big data operations\n",
    "    * Split volums of data \n",
    "    * Access data fast\n",
    "    * Distribute computations to nodes\n",
    "2. Handle fault tolerance\n",
    "    * Replicate data partitions\n",
    "    * Recover files when needed\n",
    "3. Enable adding more racks (scaling out)\n",
    "4. Optimized for specific data types\n",
    "\n",
    "\n",
    "**MapReduce** is a big data programming model that supports all the requirements of big data modeling mentioned above. It can\n",
    "* Model processing of large data, \n",
    "* Split complications into different parallel tasks,\n",
    "* Make efficient use of large commodity clusters and distributed file systems. \n",
    "\n",
    "In addition, it abstracts out the details of parallelzation, fault tolerance, data distribution, monitoring and load balancing.\n",
    "\n",
    "As a programming model, it has been implemented in a few different **big data frameworks**, for example **Hadoop**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hadoop Ecosystem\n",
    "\n",
    "Major Goals\n",
    "1. Enable Scalability\n",
    "2. Handle Fault Tolerance\n",
    "3. Optimized for a Variety Data Types\n",
    "4. Facilitate a Shared Environment\n",
    "5. Provide Value\n",
    "\n",
    "## **What**'s in the ecosystem?\n",
    "\n",
    "* 2004 - Google published a paper about their in-house processing framework they called MapReduce. \n",
    "* 2005 - Yahoo released an open-source implementation based on this framework called Hadoop.\n",
    "* Now there's over a 100 open source projects for big data.\n",
    "\n",
    "**One Possible Layer Diagram for Hadoop** (Image Credit: [The Hadoop Ecosystem: Welcome to the zoo!, UC San Diego, Coursera](https://www.coursera.org/learn/big-data-introduction/lecture/BpHNu/the-hadoop-ecosystem-welcome-to-the-zoo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"companion_files/hadoop_ecosystem.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS (Hadoop distributed file system)\n",
    "\n",
    "* HDFS is the foundation for many big data frameworks, since it provides **scaleable** and **reliable** storage. \n",
    "* **Enables Scaling** out of the resources.\n",
    "    * Achieves scalability by partitioning or splitting large files across multiple computers. \n",
    "* **Replicates** file blocks on different nodes **for fault tolerance** (by default it maintains 3 copies of every block).\n",
    "* **2 key components** of HDFS\n",
    "    * **NameNode** for metadata (usually one per cluster)\n",
    "    * **DataNode** for block storage (usually one per machine)\n",
    "\n",
    "### YARN (The resource manager for Hadoop)\n",
    "\n",
    "* Provides flexible scheduling and resource management over the HDFS storage.\n",
    "* Yarn gives you **many ways for applications** to extract value from data.\n",
    "* It lets you **run many distributed applications** over the same Hadoop cluster.\n",
    "* Yarn **reduces the need to move data** around and supports higher resource utilization resulting in lower costs.\n",
    "* Yarn is used at Yahoo to schedule jobs across 40,000 servers.\n",
    "\n",
    "### MapReduce (Simple Programming for Big Results)\n",
    "\n",
    "* Mapreduce is a programming model that simplifies parallel computing.\n",
    "* Google previously used it for indexing websites.\n",
    "\n",
    "You only need to create a **map** and **reduce** tasks, and you don't have to worry about multiple threads, synchronization, or concurrency issues. Based on functional programming  \n",
    "\n",
    "* **Map** == Apply operation to all components\n",
    "* **Reduce** == Summarize operation on elements\n",
    "\n",
    "#### WordCount, the Hellow World! of MapReduce\n",
    "\n",
    "*Image credits: [MapReduce: Simple Programming for Big Results, UC San Diego, Coursera](https://www.coursera.org/learn/big-data-introduction/lecture/pL4NH/mapreduce-simple-programming-for-big-results)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Step 0**. File is stored in HDFS. HDFS partitions the blocks across multiple nodes in the cluster.\n",
    "\n",
    "<img src=\"companion_files/mapreduce_step0.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Step 1**. **Map** on each node (Parallelization over the input)  \n",
    "Map Generates Key-value pairs for each word\n",
    "\n",
    "<img src=\"companion_files/mapreduce_step1_1.png\"/>\n",
    "\n",
    "<img src=\"companion_files/mapreduce_step1_2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Step 2**. **Sort and Shuffle**, Pairs with same key moved to same node (Parllelization over the intermediate results)\n",
    "\n",
    "<img src=\"companion_files/mapreduce_step2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Step 3**. **Reduce**, add values for same keys (Parallelization over data groups)\n",
    "\n",
    "<img src=\"companion_files/mapreduce_step3.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapReduce is Bad For\n",
    "\n",
    "* Frequently changing data\n",
    "* Dependent tasks\n",
    "* Interactive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive and Pig\n",
    "\n",
    "* Hive and Pig are two additional programming models on top of MapReduce to augment data modeling of MapReduce with **relational algebra** and **data flow modeling** respectively.\n",
    "* **Hive** was created at Facebook to issue SQL-like queries using MapReduce on their data in HDFS. \n",
    "* **Pig** was created at Yahoo to model data flow based programs using MapReduce. \n",
    "\n",
    "### Giraph\n",
    "\n",
    "* Sas built for processing large-scale graphs efficiently.\n",
    "* Facebook uses Giraph to analyze the social graphs of its users.\n",
    "\n",
    "### Storm, Spark, and Flink\n",
    "\n",
    "* Were built for **real-time (Storm)** and **in memory (Spark)** processing of big data on top of the YARN resource scheduler and HDFS.\n",
    "    * In-memory processing is a powerful way of running big data applications even faster, achieving 100x's better performance for some tasks.\n",
    "\n",
    "### **NoSQL projects** such as **Cassandra, MongoDB**, and **HBase** \n",
    "\n",
    "* Handle collections of key-values or large sparse tables. \n",
    "* **Cassandra** was created at Facebook, but Facebook also used **HBase** for its messaging platform.\n",
    "\n",
    "### Zookeeper\n",
    "\n",
    "* Zookeeper is a centralized management system for synchronization, configuration and to ensure high availability when running all of these tools.\n",
    "* It was created by Yahoo to wrangle services named after animals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Where** is Hadoop used?\n",
    "\n",
    "* Future anticipated data growth\n",
    "* Long term availability of data\n",
    "* Many platforms over single data store\n",
    "* High Volume\n",
    "* Highe Variety\n",
    "\n",
    "### The Hadoop framework is generally not the best for\n",
    "\n",
    "* Small datasets\n",
    "* Advanced Algorithms\n",
    "* Infrastructure Replacement\n",
    "* Task Level Parallelism\n",
    "* Random Data Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud Providers\n",
    "\n",
    "### infrastructure as a service (IaaS)\n",
    "\n",
    "* Iaas == Get the hardware only\n",
    "* **You**: Install and maintain OS Application Software \n",
    "* Example: Amazone EC2\n",
    "\n",
    "### Platform as a service (PaaS)\n",
    "\n",
    "* PaaS == Get the computing environment (OS, programming languages, etc.)\n",
    "* **You**: Application Software\n",
    "* Examples: Google App Engine, Microsoft Azure\n",
    "\n",
    "### Application as a service (SaaS)\n",
    "\n",
    "* SaaS == Get full software on-Demand (hardware, software)\n",
    "* **You**: Domain Goals\n",
    "* Example: Dropbox\n",
    "\n",
    "### XaaS == Anything as a Service"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
