{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvard University course, CS109 Data Science.  \n",
    "Slides1: https://github.com/cs109/2015/blob/master/Lectures/07-BiasAndRegression.pdf  \n",
    "Slides2: https://github.com/cs109/2015/blob/master/Lectures/08-RegressionContinued.pdf  \n",
    "Slides3: https://github.com/cs109/2015/blob/master/Lectures/09-ClassificationPCA.pdf  \n",
    "Slides4: https://github.com/cs109/2015/blob/master/Lectures/10-SVMAndEvaluation.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Regression\n",
    "\n",
    "### Bias Types\n",
    "* **Selection Bias**. Where did the data come from?\n",
    "* **Publication Bias**. What percentage of the scientific discoveries are replicatable? If you are to reproduce the results, will you succeed? How true are the discoveries?\n",
    "* **Non-response Bias**. What if the people who didn't answer the survey questions are an important group of people?\n",
    "* **Length Bias**. For example, you want to measure the average prison sentence. If you show up at a random point in time you would most probably see the prisoners who are going to be there for a long time and you would not meet many people who are serving 1, 2 weeks, days...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical/Statistical Definition of Bias\n",
    "\n",
    "The bias of an estimator is how far off it is on average: \n",
    "\n",
    "\\begin{equation*}\n",
    "bias(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta   \n",
    "\\end{equation*}\n",
    "\n",
    "*where $\\theta$ is what we are trying to estimate, and $\\hat{\\theta}$ is it's estimator*.\n",
    "\n",
    "The question may arise, why not subtract the bias? \n",
    "Because\n",
    "* We don't know the bias. We can try to estimate it but will have bias in that process as well.\n",
    "* **Bias_Variance Tradoff**, which is very often formulates the following way  \n",
    "\n",
    "\\begin{equation*}\n",
    "MSE(\\hat{\\theta}) = VAR(\\hat{\\theta}) + bias^2(\\hat{\\theta})\n",
    "\\end{equation*}\n",
    "\n",
    "**MSE** is the **Mean Squared Error**, the most common way to measure how good is your estimator (on average, in terms of squared distance, how far off are you from the truth?).\n",
    "\n",
    "#### So the goal is not to minimize the bias but instead the more appropriate goal is to minimize MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher Weighting\n",
    "\n",
    "How should we combine independent, *unbiased* estimators for a parameter into one estimator?\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\theta} = \\sum_{i=1}^{k}w_i\\hat{\\theta_i}\n",
    "\\end{equation*}\n",
    "\n",
    "The *weights* should sum to 1 but how should they be chosen?\n",
    "\n",
    "\\begin{equation*}\n",
    "w_i \\propto \\frac{1}{Var(\\hat{\\theta_i})}\n",
    "\\end{equation*}\n",
    "\n",
    "(Inversly proportional to variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Toward the Mean\n",
    "\n",
    "Galten investigated the heights of fathers and sons and found out that if the father is very tall the sone will be tall but not as tall as his father. And if the father is very short, the sun will be short but not as short as the father. \n",
    "\n",
    "This is the regression towards the mean and it's very common in various scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model\n",
    "\n",
    "Often called **OLS**, Ordinary Least Squares\n",
    "\n",
    "\\begin{equation*}\n",
    "y = X * \\beta + \\epsilon\n",
    "\\end{equation*}\n",
    "\n",
    "where \n",
    "* $y$ is n X 1, we want to predict\n",
    "* $X$ is n X k, matrix of data \n",
    "* $\\beta$ is k X 1, parameters\n",
    "* $\\epsilon$ is n X 1, error terms\n",
    "\n",
    "**Note**, what makes the linear models linear are parameters, so we can take $x^2$ or $x^3$ and their linear combination would still be linear.\n",
    "\n",
    "#### Coefficients\n",
    "\n",
    "* For the **sample** (1 predictor)\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\bar{x}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n",
    "\\end{equation*}\n",
    "\n",
    "* For the **population** (1 predictor)\n",
    "\n",
    "\\begin{equation*}\n",
    "y = \\beta_0 + \\beta_1 x + \\epsilon\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "E(y) = \\beta_0 + \\beta_1 E(x)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "cov(y, x) = \\beta_1 cov(x,x)\n",
    "\\end{equation*}\n",
    "\n",
    "**\"Explained\" Variance** \n",
    "\n",
    "\\begin{equation*}\n",
    "var(y) = var(X\\hat{\\beta}) + var(e)\n",
    "\\end{equation*}\n",
    "\n",
    "*where e are the residuals, the estimators of errors*.\n",
    "\n",
    "#### R-squared\n",
    "\n",
    "\\begin{equation*}\n",
    "R^2 = \\frac{var(X\\hat{\\beta})}{var(y)} = \\frac{\\sum_{i=1}^{n}(\\hat{y_i} - \\bar{y})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\n",
    "\\end{equation*}\n",
    "\n",
    "* $R^2$ is the variance explained by the fitted model divided by the total variance.\n",
    "* $R^2$ measures goodness of fit, but it doesn't validate the model.\n",
    "* Adding more predictors can only increase $R^2$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collinearity\n",
    "\n",
    "Avoid having highly correlated predictor variables (colinearity results in instability, high variances in estimates, and worse interpretability)\n",
    "\n",
    "## Odds Ratio\n",
    "\n",
    "* Odds ratio is just a different parametization of probability, if someon's probability of experiencing an outcome is $p$, then that person's **odds** of the outcome is $p/(1-p)$.\n",
    "Probability and odds are often used interchangably. For small $p$ it is almost the same, but if $p$ is large, they are different.\n",
    "* The **odds ratio** is the ratio of two different people's odds of some outcome. If people in group $A$ have the probability $p_A$ of disease, and people in group $B$ have the probability of $p_B$, then the odds ratio of group A vs. group B is \n",
    "\n",
    "\\begin{equation*}\n",
    "Odds\\ Ratio = \\frac{\\frac{p_A}{1-p_A}}{\\frac{p_B}{1-p_B}} = \\frac{(1-p_B)p_A}{(1-p_A)p_B}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Logistic Regression Model\n",
    "\n",
    "\\begin{equation*}\n",
    "logit(p) = ln\\big(\\frac{p}{1-p}\\big) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k\n",
    "\\end{equation*}\n",
    "\n",
    "* The unknown parameters $\\beta$ can be estimated from the data using **MLE** (Maximum Likelihood Estimate)\n",
    "\n",
    "#### Interpreting the parameters of logistic regression\n",
    "\n",
    "Let's say we have two patients, A and B, that are identical on all parameters. Patiant A has taken a medicine while pateient B has not. The model predicts that  \n",
    "\n",
    "$\\ $  \n",
    "$logit(p_A) = ln(\\frac{p}{1-p}) = \\beta_0 + \\beta_{age}x_{age} + ... + \\beta_kx_k + \\beta_{medicine}x_{medicine}$\n",
    "\n",
    "$logit(p_B) = ln(\\frac{p}{1-p}) = \\beta_0 + \\beta_{age}x_{age} + ... + \\beta_kx_k$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\beta_{medicine} = logit(p_A) - logit(p_B) = ln\\bigg(\\frac{\\frac{p_A}{1-p_A}}{\\frac{p_B}{1-p_B}}\\bigg)\n",
    "\\end{equation*}\n",
    "\n",
    "Using this model we can estimate an \"adjusted\" odds ratio that's the odds ratio for two people with all other known factors held constant:\n",
    "\n",
    "\\begin{equation*}\n",
    "e^{\\hat{\\beta}_{medicine}} = \\frac{\\frac{p_A}{1-p_A}}{\\frac{p_B}{1-p_B}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of Dimentionality\n",
    "\n",
    "* many features\n",
    "* in high dimention the space is so sparse that it is difficult to find a neighbor to work with so we extrapolate which is not reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "In linear regression instead of minimizing the sum of squared residuals, Ridge regression says to minimize  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{i=1}^{n}\\big(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij}\\big)^2 + \\lambda\\sum_{j=1}^{p}\\beta_j^2= RSS + \\lambda\\sum_{j=1}^{p}\\beta_j^2\n",
    "\\end{equation*}\n",
    "\n",
    "Shrinks the parameters towards the 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "In linear regression instead of minimizing the sum of squared residuals, Lasso regression says to minimize  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{i=1}^{n}\\big(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij}\\big)^2 + \\lambda\\sum_{j=1}^{p}\\big|\\beta_j\\big|= RSS + \\lambda\\sum_{j=1}^{p}\\big|\\beta_j\\big|\n",
    "\\end{equation*}\n",
    "\n",
    "This helps induce *sparsity*, reducing the number of variables one has to deal with. Sets some parameters to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors Classifiers\n",
    "\n",
    "Predict class of new data point by majority vote of K nearest neighbors.\n",
    "\n",
    "#### 1-NN Properties\n",
    "\n",
    "* Simple and quite good for low dimentional data\n",
    "* \"Rough\" decision boundary, may have \"islands\"\n",
    "* **Training complexity** for N data points? As I just everything around during the training time, to add one new data point, I just add it. So the complexity of adding a new data point is $O(1)$, for N data points it is $O(N)$\n",
    "* **Test complexity** for M data points? For every point during test time, I look at every example in training set. Hence for M test data points the complexity is $O(M*N)$\n",
    "* **Error on training set**? It's 0.\n",
    "* **Variance**? **Bias**? Variance is very high, Bias is low.\n",
    "\n",
    "#### k-NN Properties\n",
    "\n",
    "* Gets rid of \"islands\"\n",
    "* If k is too large, the boundary may become too smooth.\n",
    "* **Lower variance, increased bias.**\n",
    "* How do we choose the ideal k? Cross-validation\n",
    "\n",
    "#### Decisions to make\n",
    "\n",
    "* Choose k based on cross-validation, with the lowest test error\n",
    "* Distance measure\n",
    "* How to assign a value to a new data point?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "**Basic idea**. Project the high-dimensional data onto a lower-dimensional subspace that best \"fits\" the data\n",
    "\n",
    "### Principal Components Analysis (PCA)\n",
    "\n",
    "The algorithm\n",
    "* Subtract the mean from the data (center X) \n",
    "* (Typically) Scale each dimension by its variance\n",
    "    * Helps pay less attention to magnitude of the dimentions\n",
    "* Compute covariance matrix $S$, $S = \\frac{1}{N}X^TX$\n",
    "* Compute k largest eigenvectors of $S$\n",
    "* These eigenvectors are the k principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "\n",
    "* Widely used for all sorts of classification problems\n",
    "* Maximum margin classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Functions\n",
    "\n",
    "* With kernels we don't need to care about the actual representations of the points in a high-dimentional space. We only need the dot product.\n",
    "\n",
    "\\begin{equation*}\n",
    "K(x, z) = \\Phi(x)\\Phi(z)\n",
    "\\end{equation*}\n",
    "\n",
    "* **Polinomial** (tune $s$): \n",
    "\\begin{equation*}\n",
    "K(x, z) = (1 + x \\cdot z)^s\n",
    "\\end{equation*}\n",
    "\n",
    "* **Radial Basis Function** (tune $\\gamma$):\n",
    "\\begin{equation*}\n",
    "K(x, z) = exp(-\\gamma(x-z)^2)\n",
    "\\end{equation*}\n",
    "\n",
    "### Kernel Trick for SVM\n",
    "\n",
    "* Arbitrary many dimensions\n",
    "* Little computational cost\n",
    "* Maximum margin helps with curse of dimensionality\n",
    "\n",
    "\n",
    "### Prediction\n",
    "\n",
    "* We only need the dot product of the new data with the support vectors.\n",
    "* Prediction speed depends on the number of support vectors.\n",
    "\n",
    "### Math behind the SVMs\n",
    "\n",
    "* Andrew Ng's CS229 Machine Learning course notes: http://cs229.stanford.edu/notes/cs229-notes3.pdf\n",
    "* Andrew Ng's CS229 Machine Learning course on Youtube: https://www.youtube.com/watch?v=UzxYlbK2c7E \n",
    "\n",
    "### Tips and Tricks\n",
    "\n",
    "* SVMs are not scale invariant\n",
    "* Ckeck if the library you are using normalizes the data by default\n",
    "* Normalize the data\n",
    "    - mean 0, std 1\n",
    "    - map to [0,1] or [-1,1]\n",
    "* Normalize test set in the same way\n",
    "\n",
    "### Parameters to tune\n",
    "\n",
    "* Which kernel?\n",
    "    * RBF kernel is a good default\n",
    "* Which values for the kernel parameters\n",
    "    * try exponential sequences\n",
    "* Which value for C\n",
    "    * try exponential sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
