{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Regression\n",
    "\n",
    "Harvard University course, CS109 Data Science.  \n",
    "Slides1: https://github.com/cs109/2015/blob/master/Lectures/07-BiasAndRegression.pdf  \n",
    "Slides2: https://github.com/cs109/2015/blob/master/Lectures/08-RegressionContinued.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Types\n",
    "* **Selection Bias**. Where did the data come from?\n",
    "* **Publication Bias**. What percentage of the scientific discoveries are replicatable? If you are to reproduce the results, will you succeed? How true are the discoveries?\n",
    "* **Non-response Bias**. What if the people who didn't answer the survey questions are an important group of people?\n",
    "* **Length Bias**. For example, you want to measure the average prison sentence. If you show up at a random point in time you would most probably see the prisoners who are going to be there for a long time and you would not meet many people who are serving 1, 2 weeks, days...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical/Statistical Definition of Bias\n",
    "\n",
    "The bias of an estimator is how far off it is on average: \n",
    "\n",
    "\\begin{equation*}\n",
    "bias(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta   \n",
    "\\end{equation*}\n",
    "\n",
    "*where $\\theta$ is what we are trying to estimate, and $\\hat{\\theta}$ is it's estimator*.\n",
    "\n",
    "The question may arise, why not subtract the bias? \n",
    "Because\n",
    "* We don't know the bias. We can try to estimate it but will have bias in that process as well.\n",
    "* **Bias_Variance Tradoff**, which is very often formulates the following way  \n",
    "\n",
    "\\begin{equation*}\n",
    "MSE(\\hat{\\theta}) = VAR(\\hat{\\theta}) + bias^2(\\hat{\\theta})\n",
    "\\end{equation*}\n",
    "\n",
    "**MSE** is the **Mean Squared Error**, the most common way to measure how good is your estimator (on average, in terms of squared distance, how far off are you from the truth?).\n",
    "\n",
    "#### So the goal is not to minimize the bias but instead the more appropriate goal is to minimize MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher Weighting\n",
    "\n",
    "How should we combine independent, *unbiased* estimators for a parameter into one estimator?\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\theta} = \\sum_{i=1}^{k}w_i\\hat{\\theta_i}\n",
    "\\end{equation*}\n",
    "\n",
    "The *weights* should sum to 1 but how should they be chosen?\n",
    "\n",
    "\\begin{equation*}\n",
    "w_i \\propto \\frac{1}{Var(\\hat{\\theta_i})}\n",
    "\\end{equation*}\n",
    "\n",
    "(Inversly proportional to variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Toward the Mean\n",
    "\n",
    "Galten investigated the heights of fathers and sons and found out that if the father is very tall the sone will be tall but not as tall as his father. And if the father is very short, the sun will be short but not as short as the father. \n",
    "\n",
    "This is the regression towards the mean and it's very common in various scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model\n",
    "\n",
    "Often called **OLS**, Ordinary Least Squares\n",
    "\n",
    "\\begin{equation*}\n",
    "y = X * \\beta + \\epsilon\n",
    "\\end{equation*}\n",
    "\n",
    "where \n",
    "* $y$ is n X 1, we want to predict\n",
    "* $X$ is n X k, matrix of data \n",
    "* $\\beta$ is k X 1, parameters\n",
    "* $\\epsilon$ is n X 1, error terms\n",
    "\n",
    "**Note**, what makes the linear models linear are parameters, so we can take $x^2$ or $x^3$ and their linear combination would still be linear.\n",
    "\n",
    "#### Coefficients\n",
    "\n",
    "* For the **sample** (1 predictor)\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\bar{x}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n",
    "\\end{equation*}\n",
    "\n",
    "* For the **population** (1 predictor)\n",
    "\n",
    "\\begin{equation*}\n",
    "y = \\beta_0 + \\beta_1 x + \\epsilon\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "E(y) = \\beta_0 + \\beta_1 E(x)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "cov(y, x) = \\beta_1 cov(x,x)\n",
    "\\end{equation*}\n",
    "\n",
    "**\"Explained\" Variance** \n",
    "\n",
    "\\begin{equation*}\n",
    "var(y) = var(X\\hat{\\beta}) + var(e)\n",
    "\\end{equation*}\n",
    "\n",
    "*where e are the residuals, the estimators of errors*.\n",
    "\n",
    "#### R-squared\n",
    "\n",
    "\\begin{equation*}\n",
    "R^2 = \\frac{var(X\\hat{\\beta})}{var(y)} = \\frac{\\sum_{i=1}^{n}(\\hat{y_i} - \\bar{y})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\n",
    "\\end{equation*}\n",
    "\n",
    "* $R^2$ is the variance explained by the fitted model divided by the total variance.\n",
    "* $R^2$ measures goodness of fit, but it doesn't validate the model.\n",
    "* Adding more predictors can only increase $R^2$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collinearity\n",
    "\n",
    "Avoid having highly correlated predictor variables (colinearity results in instability, high variances in estimates, and worse interpretability)\n",
    "\n",
    "#### Odds Ratio\n",
    "\n",
    "* Odds ratio is just a different parametization of probability, if someon's probability of experiencing an outcome is $p$, then that person's **odds** of the outcome is $p/(1-p)$.\n",
    "Probability and odds are often used interchangably. For small $p$ it is almost the same, but if $p$ is large, they are different.\n",
    "* The **odds ratio** is the ratio of two different people's odds of some outcome. If people in group $A$ have the probability $p_A$ of disease, and people in group $B$ have the probability of $p_B$, then the odds ratio of group A vs. group B is \n",
    "\n",
    "\\begin{equation*}\n",
    "Odds\\ Ratio = \\frac{\\frac{p_A}{1-p_A}}{\\frac{p_B}{1-p_B}} = \\frac{(1-p_B)p_A}{(1-p_A)p_B}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Logistic Regression Model\n",
    "\n",
    "\\begin{equation*}\n",
    "logit(p) = ln\\big(\\frac{p}{1-p}\\big) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k\n",
    "\\end{equation*}\n",
    "\n",
    "* The unknown parameters $\\beta$ can be estimated from the data using **MLE** (Maximum Likelihood Estimate)\n",
    "\n",
    "#### Interpreting the parameters of logistic regression\n",
    "\n",
    "Let's say we have two patients, A and B, that are identical on all parameters. Patiant A has taken a medicine while pateient B has not. The model predicts that  \n",
    "\n",
    "$\\ $  \n",
    "$logit(p_A) = ln(\\frac{p}{1-p}) = \\beta_0 + \\beta_{age}x_{age} + ... + \\beta_kx_k + \\beta_{medicine}x_{medicine}$\n",
    "\n",
    "$logit(p_B) = ln(\\frac{p}{1-p}) = \\beta_0 + \\beta_{age}x_{age} + ... + \\beta_kx_k$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\beta_{medicine} = logit(p_A) - logit(p_B) = ln\\bigg(\\frac{\\frac{p_A}{1-p_A}}{\\frac{p_B}{1-p_B}}\\bigg)\n",
    "\\end{equation*}\n",
    "\n",
    "Using this model we can estimate an \"adjusted\" odds ratio that's the odds ratio for two people with all other known factors held constant:\n",
    "\n",
    "\\begin{equation*}\n",
    "e^{\\hat{\\beta}_{medicine}} = \\frac{\\frac{p_A}{1-p_A}}{\\frac{p_B}{1-p_B}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of Dimentionality\n",
    "\n",
    "* many features\n",
    "* in high dimention the space is so sparse that it is difficult to find a neighbor to work with so we extrapolate which is not reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "In linear regression instead of minimizing the sum of squared residuals, Ridge regression says to minimize  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{i=1}^{n}\\big(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij}\\big)^2 + \\lambda\\sum_{j=1}^{p}\\beta_j^2= RSS + \\lambda\\sum_{j=1}^{p}\\beta_j^2\n",
    "\\end{equation*}\n",
    "\n",
    "Shrinks the parameters towards the 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "In linear regression instead of minimizing the sum of squared residuals, Lasso regression says to minimize  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{i=1}^{n}\\big(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij}\\big)^2 + \\lambda\\sum_{j=1}^{p}\\big|\\beta_j\\big|= RSS + \\lambda\\sum_{j=1}^{p}\\big|\\beta_j\\big|\n",
    "\\end{equation*}\n",
    "\n",
    "This helps induce *sparsity*, reducing the number of variables one has to deal with. Sets some parameters to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
