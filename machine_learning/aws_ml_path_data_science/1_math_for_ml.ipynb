{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math for Machine Learning\n",
    "\n",
    "## What is Machine Learning?\n",
    "\n",
    "Arthur Aamuel, 1959, \"Field of study that gives computers the abiity to learn without being explicitly programmed\".\n",
    "\n",
    "## The Machine Learning Pipeline in Mathematics\n",
    "\n",
    "1. **Data Processing**\n",
    "    * Uses linear algebra to format the data in a way algorithms can ingest.\n",
    "2. **Feature Engineering and Selection**\n",
    "    * Uses vectores and matrices to transform data to make it easy for algorithms to understand.\n",
    "3. **Modeling**\n",
    "    * Uses geometry, probability, norms, and statistics to define the problem in a way the algorithm can optimize.\n",
    "4. **Optimization**\n",
    "    * Uses vector calculus to interate until certain conditions are met. Then you choose the best model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors\n",
    "\n",
    "### Norm\n",
    "\n",
    "A measure of distance. \n",
    "\n",
    "### Norm Properties\n",
    "\n",
    "1. All distances are non-negative. $\\Vert \\vec{v} \\Vert \\geq 0$\n",
    "2. Distances multiply with scalar multiplication. $\\Vert a \\vec{v} \\Vert = \\vert a \\vert \\cdot \\Vert v \\Vert$\n",
    "3. *Triangle Inequality*. If I travel from $A$ to $B$ then $B$ to $C$, that is at least as far as going from $A$ to $C$. $\\Vert \\vec{v} + \\vec{w} \\Vert \\leq \\Vert \\vec{v} \\Vert + \\Vert \\vec{w} \\Vert$. \n",
    "    * If $A$, $B$, and $C$ all lie on the same line, then $\\Vert \\vec{v} + \\vec{w} \\Vert = \\Vert \\vec{v} \\Vert + \\Vert \\vec{w} \\Vert$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "### Types of Norms\n",
    "\n",
    "For $\\vec{v} = \\begin{pmatrix} v_1\\\\ v_2\\\\ ...\\\\ v_n\\end{pmatrix}$\n",
    "\n",
    "1. **Euclidean Norm**. \n",
    "\\begin{align*}\n",
    "\\Vert \\vec{v} \\Vert_2 &= \\sqrt{v_1^2 + v_2^2 + ... + v_n^2} \\\\\n",
    "                      &= \\sqrt{\\sum_{i=1}^{n} {v_i^2}}\n",
    "\\end{align*}\n",
    "2. $L_p-\\text{Norm}$ \n",
    "\\begin{equation*}\n",
    "\\Vert \\vec{v} \\Vert_p = \\Big( \\sum_{i=1}^{n} \\vert v_i \\vert ^p \\Big)^{1/p}\n",
    "\\end{equation*}\n",
    "3. $L_1-\\text{Norm}$\n",
    "\\begin{equation*}\n",
    "\\Vert \\vec{v} \\Vert_1 = \\sum_{i=1}^{n} \\vert v_i \\vert \n",
    "\\end{equation*}\n",
    "Other names are TAXICAB METRIC, MANHATTAN NORM, ...\n",
    "4. $L_\\infty-\\text{Norm}$\n",
    "\\begin{equation*}\n",
    "\\Vert \\vec{v} \\Vert_\\infty =  \\lim_{p \\to \\infty} \\Vert \\vec{v} \\Vert_p = \\lim_{p \\to \\infty} \\Big( \\sum_{i=1}^{n} \\vert v_i \\vert ^p \\Big)^{1/p} \n",
    "\\end{equation*}\n",
    "5. $L_0-\\text{Norm}$, which is not a norm, is the number of non-zero elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors in Code\n",
    "\n",
    "* We have used LaTeX to write down mathematical equations.\n",
    "* Let's use Python now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this defines a row\n",
    "v = [1, 2, 3]\n",
    "w = [1, 1, 1]\n",
    "\n",
    "# this defines a matrix\n",
    "A = [[1, 2, 3], [-1, 0, 1], [1, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 1, 1, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v + w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(v) + np.array(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 1, 2, 3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*np.array(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "3.7416573867739413\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "# L_p-Norms\n",
    "\n",
    "print(np.linalg.norm(v, ord=1))\n",
    "print(np.linalg.norm(v, ord=2))\n",
    "print(np.linalg.norm(v, ord=np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "\n",
    "### Linear Algebra Operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot Products\n",
    "\n",
    "If we are given two vectors, $\\vec{v} = \\begin{pmatrix} v_1\\\\ v_2\\\\ ...\\\\ v_n\\end{pmatrix}$, $\\vec{w} = \\begin{pmatrix} w_1\\\\ w_2\\\\ ...\\\\ w_n\\end{pmatrix}$, then their dot product is  \n",
    "\n",
    "\\begin{align*}\n",
    "\\vec{v} \\cdot \\vec{w} &= \\vec{v}^T \\vec{w} = \\begin{pmatrix} v_1 v_2 ... v_n\\end{pmatrix} \\begin{pmatrix} w_1\\\\ w_2\\\\ ...\\\\ w_n\\end{pmatrix}\\\\\n",
    "                      &= \\sum_{i=1}^{n} v_iw_i\n",
    "\\end{align*}\n",
    "\n",
    "If we view this in terms of angles, the angle between $\\vec{v}$ and $\\vec{w}$, $\\theta$, is equal to\n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta = \\arccos{\\frac{\\vec{v} \\cdot \\vec{w}}{\\Vert \\vec{v} \\Vert \\Vert \\vec{w} \\Vert}}\n",
    "\\end{equation*}\n",
    "\n",
    "Or\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{v} \\cdot \\vec{w} = \\Vert \\vec{v} \\Vert \\Vert \\vec{w} \\Vert \\cos{\\theta}\n",
    "\\end{equation*}\n",
    "\n",
    "* **Orthogonality**. $\\vec{v} \\cdot \\vec{w} = 0$, assuming $\\vec{v} \\neq 0, \\vec{w} \\neq 0$.  \n",
    "$\\vec{v} \\cdot \\vec{w}$ will be $0$ when $\\cos{\\theta} = 0$, which happens when $\\theta = -\\pi/2$ $(-90^\\circ)$ or $\\theta = \\pi/2$  $(90^\\circ)$. \n",
    "The intuition of being orthogonal works in any dimension.\n",
    "\n",
    "* $\\vec{v} \\cdot \\vec{w} > 0$, assuming $\\vec{v} \\neq 0, \\vec{w} \\neq 0$.  \n",
    "$\\vec{v} \\cdot \\vec{w}$ will be greater than $0$ when $\\cos{\\theta} > 0$, which happens when $-\\pi/2 < \\theta < \\pi/2$. In other words, all vectors pointing somewhat the same direction from the orthogonal line/plane/etc have a dot product greater than $0$.\n",
    "\n",
    "* $\\vec{v} \\cdot \\vec{w} < 0$, assuming $\\vec{v} \\neq 0, \\vec{w} \\neq 0$.  \n",
    "$\\vec{v} \\cdot \\vec{w}$ will be less than $0$ when $\\cos{\\theta} < 0$, which happens when $\\theta < -\\pi/2$ or $\\theta > \\pi/2$. In other words, all vectors pointing somewhat the opposite direction from the orthogonal line/plane/etc have a dot product less than $0$.\n",
    "\n",
    "This gives us the way we can understand geometrically dot products. And what it leads us to, is the notion of hyperplane.  \n",
    "\n",
    "#### Hyperplane Definition\n",
    "\n",
    "* It is the thing orthogonal to a given vector.\n",
    "* In 2D, it is the line orthogonal to a given vector.\n",
    "* In 3D, it is the plane orthogonal to a given vector.\n",
    "* In higher dimensions, similar. For example, in 4D it is the 3D space orthogonal to a given vector, etc.\n",
    "\n",
    "Notice that hyperplanes pass through the origin ($(0, 0, 0)$ in 3D for example). We can extend the above definition by adding \"or a translate to a different point\". Thus, the hyperplane can also pass through another place, different from the origin.  \n",
    "\n",
    "So this is the geometric notion of a hyperplane. It's just some subspace of your given high dimentional space that separates it into two equal parts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication\n",
    "\n",
    "If we are given a weight matrix, $W = \\begin{pmatrix} - \\vec{w_1} -\\\\ - \\vec{w_2} -\\\\ ...\\\\ - \\vec{w_m} -\\end{pmatrix}$, and a feature vector, $\\vec{v} = \\begin{pmatrix} v_1\\\\ v_2\\\\ ...\\\\ v_n\\end{pmatrix}$, then   \n",
    "\n",
    "\\begin{align*}\n",
    "W \\vec{v} &= \\begin{pmatrix} - \\vec{w_1} -\\\\ - \\vec{w_2} -\\\\ ...\\\\ - \\vec{w_m} -\\end{pmatrix} \\begin{pmatrix} v_1\\\\ v_2\\\\ ...\\\\ v_n\\end{pmatrix} \\\\\n",
    "         &= \\begin{pmatrix} \\vec{w_1} \\vec{v}\\\\ \\vec{w_2} \\vec{v}\\\\ ...\\\\ \\vec{w_m} \\vec{v}\\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^{n} w_{1i}{v_i}\\\\ \\sum_{i=1}^{n} w_{2i}{v_i}\\\\ ...\\\\ \\sum_{i=1}^{n} w_{mi}{v_i}\\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Now, if we have a weight matrix, $W = \\begin{pmatrix} - \\vec{w_1} -\\\\ - \\vec{w_2} -\\\\ ...\\\\ - \\vec{w_m} -\\end{pmatrix}$, and a feature matrix, $X = \\begin{pmatrix} \\vec{v_1} \\vec{v_2} ... \\vec{v_k} \\end{pmatrix}$, then\n",
    "\n",
    "\\begin{align*}\n",
    "W X &= \\begin{pmatrix} - \\vec{w_1} -\\\\ - \\vec{w_2} -\\\\ ...\\\\ - \\vec{w_m} -\\end{pmatrix} \\begin{pmatrix} \\vec{v_1} \\vec{v_2} ... \\vec{v_k} \\end{pmatrix} \\\\\n",
    "         &= \\begin{pmatrix} \\vec{w_1} \\vec{v_1} \\quad \\vec{w_1} \\vec{v_2} \\quad ... \\quad \\vec{w_1} \\vec{v_k}\\\\ \\vec{w_2} \\vec{v_1} \\quad \\vec{w_2} \\vec{v_2} \\quad ... \\quad \\vec{w_2} \\vec{v_k}\\\\ \\quad ... \\quad \\\\ \\vec{w_m} \\vec{v_1} \\quad \\vec{w_m} \\vec{v_2} \\quad ... \\quad \\vec{w_m} \\vec{v_k} \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Thus, the $i, j$-th element of the matrix $WX$ will correspond to the $i$-th feature of the $j$-th data point.  \n",
    "\n",
    "**Formal Definition**\n",
    "\n",
    "If $A$ is a matrix where the rows are the features $w_i$ and $B$ is a matrix where the columns are data vectors $v_j$ then the $i,j$-th entry of the $C = AB$ product is $w_iv_j$, which is to say the $i$-th feature of the $j$-th vector. \n",
    " \n",
    "$$\\boxed{\n",
    "\\begin{equation}\n",
    "c_{i,j} = \\sum_l a_{i,l} b_{l,j}\n",
    "\\end{equation}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication and examples\n",
    "\n",
    "Hadamard product\n",
    "\n",
    "Matrix product properties\n",
    "\n",
    "Geometry of matrix operations\n",
    "\n",
    "Determinant computation\n",
    "\n",
    "Matrix invertibility\n",
    "\n",
    "Linear dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
