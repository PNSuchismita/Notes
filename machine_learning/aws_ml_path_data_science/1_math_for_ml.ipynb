{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math for Machine Learning, AWS Machine Learning: Data Scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?\n",
    "\n",
    "Arthur Aamuel, 1959, \"Field of study that gives computers the abiity to learn without being explicitly programmed\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Pipeline in Mathematics\n",
    "\n",
    "1. **Data Processing**\n",
    "    * Uses linear algebra to format the data in a way algorithms can ingest.\n",
    "2. **Feature Engineering and Selection**\n",
    "    * Uses vectores and matrices to transform data to make it easy for algorithms to understand.\n",
    "3. **Modeling**\n",
    "    * Uses geometry, probability, norms, and statistics to define the problem in a way the algorithm can optimize.\n",
    "4. **Optimization**\n",
    "    * Uses vector calculus to interate until certain conditions are met. Then you choose the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors\n",
    "\n",
    "### Norm\n",
    "\n",
    "A measure of distance. \n",
    "\n",
    "### Norm Properties\n",
    "\n",
    "1. All distances are non-negative. $\\Vert \\vec{v} \\Vert \\geq 0$\n",
    "2. Distances multiply with scalar multiplication. $\\Vert a \\vec{v} \\Vert = \\vert a \\vert \\cdot \\Vert v \\Vert$\n",
    "3. *Triangle Inequality*. If I travel from $A$ to $B$ then $B$ to $C$, that is at least as far as going from $A$ to $C$. $\\Vert \\vec{v} + \\vec{w} \\Vert \\leq \\Vert \\vec{v} \\Vert + \\Vert \\vec{w} \\Vert$. \n",
    "    * If $A$, $B$, and $C$ all lie on the same line, then $\\Vert \\vec{v} + \\vec{w} \\Vert = \\Vert \\vec{v} \\Vert + \\Vert \\vec{w} \\Vert$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "### Types of Norms\n",
    "\n",
    "For $\\vec{v} = \\begin{pmatrix} v_1\\\\ v_2\\\\ ...\\\\ v_n\\end{pmatrix}$\n",
    "\n",
    "* **Euclidean Norm**.  \n",
    "\n",
    "\\begin{align*}\n",
    "\\Vert \\vec{v} \\Vert_2 &= \\sqrt{v_1^2 + v_2^2 + ... + v_n^2} \\\\\n",
    "                      &= \\sqrt{\\sum_{i=1}^{n} {v_i^2}}\n",
    "\\end{align*}\n",
    "\n",
    "* $L_p-\\text{Norm}$ \n",
    "\n",
    "\\begin{equation*}\n",
    "\\Vert \\vec{v} \\Vert_p = \\Big( \\sum_{i=1}^{n} \\vert v_i \\vert ^p \\Big)^{1/p}\n",
    "\\end{equation*}\n",
    "\n",
    "* $L_1-\\text{Norm}$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Vert \\vec{v} \\Vert_1 = \\sum_{i=1}^{n} \\vert v_i \\vert \n",
    "\\end{equation*}\n",
    "\n",
    "Other names are TAXICAB METRIC, MANHATTAN NORM, ...\n",
    "\n",
    "* $L_\\infty-\\text{Norm}$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Vert \\vec{v} \\Vert_\\infty =  \\lim_{p \\to \\infty} \\Vert \\vec{v} \\Vert_p = \\lim_{p \\to \\infty} \\Big( \\sum_{i=1}^{n} \\vert v_i \\vert ^p \\Big)^{1/p} \n",
    "\\end{equation*}\n",
    "\n",
    "* $L_0-\\text{Norm}$, which is not a norm, is the number of non-zero elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors in Code\n",
    "\n",
    "* We have used LaTeX to write down mathematical equations.\n",
    "* Let's use Python now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this defines a row\n",
    "v = [1, 2, 3]\n",
    "w = [1, 1, 1]\n",
    "\n",
    "# this defines a matrix\n",
    "A = [[1, 2, 3], [-1, 0, 1], [1, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 1, 1, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v + w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(v) + np.array(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 1, 2, 3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*np.array(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "3.7416573867739413\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "# L_p-Norms\n",
    "\n",
    "print(np.linalg.norm(v, ord=1))\n",
    "print(np.linalg.norm(v, ord=2))\n",
    "print(np.linalg.norm(v, ord=np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "\n",
    "### Linear Algebra Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Products\n",
    "\n",
    "If we are given two vectors, $\\vec{v} = \\begin{pmatrix} v_1\\\\ v_2\\\\ ...\\\\ v_n\\end{pmatrix}$, $\\vec{w} = \\begin{pmatrix} w_1\\\\ w_2\\\\ ...\\\\ w_n\\end{pmatrix}$, then their dot product is  \n",
    "\n",
    "\\begin{align*}\n",
    "\\vec{v} \\cdot \\vec{w} &= \\vec{v}^T \\vec{w} = \\begin{pmatrix} v_1 v_2 ... v_n\\end{pmatrix} \\begin{pmatrix} w_1\\\\ w_2\\\\ ...\\\\ w_n\\end{pmatrix}\\\\\n",
    "                      &= \\sum_{i=1}^{n} v_iw_i\n",
    "\\end{align*}\n",
    "\n",
    "If we view this in terms of angles, the angle between $\\vec{v}$ and $\\vec{w}$, $\\theta$, is equal to\n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta = \\arccos{\\frac{\\vec{v} \\cdot \\vec{w}}{\\Vert \\vec{v} \\Vert \\Vert \\vec{w} \\Vert}}\n",
    "\\end{equation*}\n",
    "\n",
    "Or\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{v} \\cdot \\vec{w} = \\Vert \\vec{v} \\Vert \\Vert \\vec{w} \\Vert \\cos{\\theta}\n",
    "\\end{equation*}\n",
    "\n",
    "* **Orthogonality**. $\\vec{v} \\cdot \\vec{w} = 0$, assuming $\\vec{v} \\neq 0, \\vec{w} \\neq 0$.  \n",
    "$\\vec{v} \\cdot \\vec{w}$ will be $0$ when $\\cos{\\theta} = 0$, which happens when $\\theta = -\\pi/2$ $(-90^\\circ)$ or $\\theta = \\pi/2$  $(90^\\circ)$. \n",
    "The intuition of being orthogonal works in any dimension.\n",
    "\n",
    "* $\\vec{v} \\cdot \\vec{w} > 0$, assuming $\\vec{v} \\neq 0, \\vec{w} \\neq 0$.  \n",
    "$\\vec{v} \\cdot \\vec{w}$ will be greater than $0$ when $\\cos{\\theta} > 0$, which happens when $-\\pi/2 < \\theta < \\pi/2$. In other words, all vectors pointing somewhat the same direction from the orthogonal line/plane/etc have a dot product greater than $0$.\n",
    "\n",
    "* $\\vec{v} \\cdot \\vec{w} < 0$, assuming $\\vec{v} \\neq 0, \\vec{w} \\neq 0$.  \n",
    "$\\vec{v} \\cdot \\vec{w}$ will be less than $0$ when $\\cos{\\theta} < 0$, which happens when $\\theta < -\\pi/2$ or $\\theta > \\pi/2$. In other words, all vectors pointing somewhat the opposite direction from the orthogonal line/plane/etc have a dot product less than $0$.\n",
    "\n",
    "This gives us the way we can understand geometrically dot products. And what it leads us to, is the notion of hyperplane.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperplane Definition\n",
    "\n",
    "* It is the thing orthogonal to a given vector.\n",
    "* In 2D, it is the line orthogonal to a given vector.\n",
    "* In 3D, it is the plane orthogonal to a given vector.\n",
    "* In higher dimensions, similar. For example, in 4D it is the 3D space orthogonal to a given vector, etc.\n",
    "\n",
    "Notice that hyperplanes pass through the origin ($(0, 0, 0)$ in 3D for example). We can extend the above definition by adding \"or a translate to a different point\". Thus, the hyperplane can also pass through another place, different from the origin.  \n",
    "\n",
    "So this is the geometric notion of a hyperplane. It's just some subspace of your given high dimentional space that separates it into two equal parts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication\n",
    "\n",
    "If we are given a weight matrix, $W = \\begin{pmatrix} - \\vec{w_1} -\\\\ - \\vec{w_2} -\\\\ ...\\\\ - \\vec{w_m} -\\end{pmatrix}$, and a feature vector, $\\vec{v} = \\begin{pmatrix} v_1\\\\ v_2\\\\ ...\\\\ v_n\\end{pmatrix}$, then   \n",
    "\n",
    "\\begin{align*}\n",
    "W \\vec{v} &= \\begin{pmatrix} - \\vec{w_1} -\\\\ - \\vec{w_2} -\\\\ ...\\\\ - \\vec{w_m} -\\end{pmatrix} \\begin{pmatrix} v_1\\\\ v_2\\\\ ...\\\\ v_n\\end{pmatrix} \\\\\n",
    "         &= \\begin{pmatrix} \\vec{w_1} \\vec{v}\\\\ \\vec{w_2} \\vec{v}\\\\ ...\\\\ \\vec{w_m} \\vec{v}\\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^{n} w_{1i}{v_i}\\\\ \\sum_{i=1}^{n} w_{2i}{v_i}\\\\ ...\\\\ \\sum_{i=1}^{n} w_{mi}{v_i}\\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Now, if we have a weight matrix, $W = \\begin{pmatrix} - \\vec{w_1} -\\\\ - \\vec{w_2} -\\\\ ...\\\\ - \\vec{w_m} -\\end{pmatrix}$, and a feature matrix, $X = \\begin{pmatrix} \\vec{v_1} \\vec{v_2} ... \\vec{v_k} \\end{pmatrix}$, then\n",
    "\n",
    "\\begin{align*}\n",
    "W X &= \\begin{pmatrix} - \\vec{w_1} -\\\\ - \\vec{w_2} -\\\\ ...\\\\ - \\vec{w_m} -\\end{pmatrix} \\begin{pmatrix} \\vec{v_1} \\vec{v_2} ... \\vec{v_k} \\end{pmatrix} \\\\\n",
    "         &= \\begin{pmatrix} \\vec{w_1} \\vec{v_1} \\quad \\vec{w_1} \\vec{v_2} \\quad ... \\quad \\vec{w_1} \\vec{v_k}\\\\ \\vec{w_2} \\vec{v_1} \\quad \\vec{w_2} \\vec{v_2} \\quad ... \\quad \\vec{w_2} \\vec{v_k}\\\\ \\quad ... \\quad \\\\ \\vec{w_m} \\vec{v_1} \\quad \\vec{w_m} \\vec{v_2} \\quad ... \\quad \\vec{w_m} \\vec{v_k} \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Thus, the $i, j$-th element of the matrix $WX$ will correspond to the $i$-th feature of the $j$-th data point.  \n",
    "\n",
    "#### Formal Definition\n",
    "\n",
    "If $A$ is a matrix where the rows are the features $w_i$ and $B$ is a matrix where the columns are data vectors $v_j$ then the $i,j$-th entry of the $C = AB$ product is $w_iv_j$, which is to say the $i$-th feature of the $j$-th vector. \n",
    " \n",
    "$$\\boxed{\n",
    "\\begin{equation}\n",
    "c_{i,j} = \\sum_l a_{i,l} b_{l,j}\n",
    "\\end{equation}}$$\n",
    "\n",
    "Notice that $c_{i,j}$ is the dot product of the $i$-th row with the $j$-th column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadamard Product\n",
    "\n",
    "**Definition**. An (often less useful) method of multiplying matrices is **element-wise**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Product Properties\n",
    "\n",
    "* **Distributivity**. $A(B+C) = AB + AC$\n",
    "* **Associativity**. $A(BC) = (AB)C$\n",
    "* **Not Commutativity**. $AB \\neq BA$\n",
    "    * Remember that matrix multiplication is commutative if - and only when - both of the matrices are diagonal and of equal dimensions.\n",
    "    \n",
    "**Data Transformation** \n",
    "* There is no problem with transforming your original data, then **transforming the transformed data**\n",
    "* However, the transformation that worked on the transformed data **will not necessarily work** on the original data. \n",
    "    \n",
    "### The Identity Matrix\n",
    "\n",
    "The identity matrix, $I$ has all ones on the diagonal and zeroes everywhere else.\n",
    "\n",
    "$I = \\begin{pmatrix} 1 \\quad 0 \\quad 0 \\quad ... \\quad 0  \\\\ 0 \\quad 1 \\quad 0 \\quad ... \\quad 0 \\\\ 0 \\quad 0 \\quad 1 \\quad ... \\quad 0 \\\\ ... \\\\ 0 \\quad 0 \\quad 0 \\quad ... \\quad 1 \\end{pmatrix}$\n",
    "\n",
    "Thus, for any matrix $A$, $\\boxed{IA = A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadamard Product Properties\n",
    "\n",
    "* **Distributivity**. $A\\circ(B+C) = A\\circ B + A\\circ C$\n",
    "* **Associativity**. $A\\circ(B\\circ C) = (A\\circ B)\\circ C$\n",
    "* **Commutativity**. $A\\circ B = B\\circ A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinant\n",
    "\n",
    "Determinant of a matrix A, det(A), is the scaling factor. It is the factor the area is multiplied by. det(A) is negative if it flips the plane over. \n",
    "\n",
    "### Matrix invertibility\n",
    "\n",
    "When you take your transformation and undo it to get your data back.\n",
    "\n",
    "* **When can you invert?**. As long as the determinant is not 0, we can invert. And when the determinant is 0, we cannot invert.\n",
    "* **How to compute the inverse**.\n",
    "Let's say we have $A = \\begin{pmatrix} a \\quad b \\\\ c \\quad d\\end{pmatrix}$ and we want to find its invers, $A^{-1} = \\begin{pmatrix} e \\quad f \\\\ g \\quad h\\end{pmatrix}$. This means that $A^{-1}A = I$.\n",
    "\n",
    "$\\begin{pmatrix} e \\quad f \\\\ g \\quad h \\end{pmatrix}$ $\\begin{pmatrix} a \\quad b \\\\ c \\quad d \\end{pmatrix}$ = $\\begin{pmatrix} 1 \\quad 0 \\\\ 0 \\quad 1 \\end{pmatrix}$\n",
    "\n",
    "Which is the same as \n",
    "\n",
    "\\begin{cases}\n",
    "ea + fc = 1 \\\\\n",
    "eb + fd = 0 \\\\\n",
    "ga + hc = 0 \\\\\n",
    "gb + hd = 1 \n",
    "\\end{cases}\n",
    "\n",
    "The solution of which is $A^{-1} = \\frac{1}{det(A)} \\begin{pmatrix} d \\quad -b \\\\ -c \\quad a \\end{pmatrix}$ = $\\frac{1}{ad - bc} \\begin{pmatrix} d\\quad-b \\\\ -c\\quad a \\end{pmatrix}$.  \n",
    "\n",
    "As we go to a higher dimentions, this becomes increasingly difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear dependence\n",
    "\n",
    "* **Definition**. $\\vec{v_1}, \\vec{v_2}, ... \\vec{v_k}$ are linearly dependent (live in $<k$ dimensions) if there exist $a_1, a_2, ... , a_k$ not all $0$ such that\n",
    "\n",
    "\\begin{equation*} \n",
    "a_1 \\vec{v_1} + a_2 \\vec{v_2} + ... + a_k \\vec{v_k} = \\vec{0}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The probability of an event is the expected fraction of time that the outcome would occur with repeated experiments.**\n",
    "\n",
    "The probability theory is used in machine learning models where the outcomes are:\n",
    "* Inherently random\n",
    "* Simply too complex to be completely known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Terminology\n",
    "\n",
    "* **Outcome**: A single possibility from the experiment.\n",
    "* **Sample Space**, $\\Omega$ (Omega): The set of all possible outcomes.\n",
    "* **Event**, $E$: Something you can observe. (with a yes/no answer)\n",
    "* **Probability**, $P(E)$: Fraction of an experiment where an event occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Axioms of Probability \n",
    "\n",
    "1. The fraction of the times an event occurs is between $0$ and $1$.\n",
    "    * $P(E) \\in [0, 1]$\n",
    "2. Something always happens. \n",
    "    * $P(\\Omega) = 1$\n",
    "3. If two events can't happen at the same time, then the fraction of the time that at least one of them occurs is the sum of the fraction of the time either one occurs separately.\n",
    "    * If we have two disjoint events, $E_1 \\cap E_2 = \\emptyset$ (empty set), then $P(E_1 \\cup E_2) = P(E_1) + P(E_2)$\n",
    "    * Works for any number of disjoint events. $P(\\cup_{i}^{} E_i) = \\sum_{i}^{} P(E_i)$\n",
    "    \n",
    "**Inclusion-Exclusion Principle**. For two events, $E_1$ and $E_2$, $P(E_1 \\cup E_2) = P(E_1) + P(E_2) - P(E_1 \\cap E_2)$. For three events, $E_1, E_2, E_3$, $P(E_1 \\cup E_2 \\cup E_3) = P(E_1) + P(E_2) + P(E_3) - P(E_1 \\cap E_2) - P(E_1 \\cap E_3) - P(E_2 \\cap E_3) + P(E_1 \\cap E_2 \\cap E_3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability \n",
    "\n",
    "The probability of $E_1$ given $E_2$ is:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(E_1 | E_2) = \\frac{P(E_1 \\cap E_2)}{P(E_2)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Rule\n",
    "\n",
    "Bayes' Rule can be leveraged to understand **competing hypotheses**. \n",
    "\n",
    "\\begin{equation*}\n",
    "P(\\text{outcome } A_1 \\text{of variable } 1 | \\text{outcome } B \\text{ of variable } 2) = \\frac{P(B|A_1)P(A_1)}{P(B|A_1)P(A_1) + P(B|A_2)P(A_2) + ... + P(B|A_k)P(A_k)}\n",
    "\\end{equation*}\n",
    "\n",
    "where $A_1, A_2, ..., A_k$ represent all other possible outcomes of the first variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
