{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear and Logistic Regression, AWS Machine Learning: Data Scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression via Least Squares Minimization\n",
    "\n",
    "### Types of Machine Learning\n",
    "\n",
    "* **Supervised**. Learns a function that approximates the relationship between input and output data\n",
    "* **Unsupervised**. Finds structure in data without using explicit labels.\n",
    "    * **Classification**. Predicts discrete labels/categories. Example, Logistic Regression\n",
    "    * **Regression**. Predicts continuous values. Example, Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Linear Models\n",
    "\n",
    "* **What is it?**. In linear modeling, the relationship between each individual input variables and the output is a straight line. Slopes of such lines become the coefficients of the linear equation.\n",
    "* **An example of a linear notation**. $y_i = a_0 + a_1x_1 + a_2x_2 + ...$.\n",
    "* **Why use linear models?**.     \n",
    "    * Interpretable\n",
    "    * Low complexity\n",
    "    * Scalable\n",
    "    * Good baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of 1-Dimension Ordinary Least Squares (OLS)\n",
    "\n",
    "* $i$ - an observation. Assume $i = 1, 2, ..., N$.\n",
    "* $x$ - independent variable, feature\n",
    "* $y$ - dependent variable, output \n",
    "\n",
    "The linear relationship looks like this\n",
    "\n",
    "\\begin{equation*}\n",
    "y = ax + b\n",
    "\\end{equation*}\n",
    "\n",
    "where $a, b$ are constants.  \n",
    "\n",
    "The OLS problem is to solve for $a$ and $b$ in order to achieve $y_i = ax_i + b + \\epsilon$, where $\\epsilon$ is the noise (there is always a noise/error: no matter how well we find $a$ and $b$, the indipendent variable never fully explains the dependent variable). We usually skip $\\epsilon$ and instead write\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y}_i = ax_i + b\n",
    "\\end{equation*}\n",
    "\n",
    "where  $\\hat{y}_i$ are our estimates of $y_i$.\n",
    "\n",
    "We solve for $a$ and $b$ by defining a loss function. A loss function is used whenever we are trying to optimize a given function. Let's define a loss function for linear regression.\n",
    "\n",
    "\\begin{equation*}\n",
    "L = \\sum_{i=1}^{N}(y_i - \\hat{y_i})^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution of Ordinary Least Squares (OLS)\n",
    "\n",
    "Find the $L$ minimum by equating the first derivatives of $L$ to zero, and solving for the values of $a$ and $b$ for which $L$ is minimum.\n",
    "\n",
    "\\begin{equation*}\n",
    "L = \\sum_{i=1}^{N}(y_i - \\hat{y_i})^2 = \\sum_{i=1}^{N}(y_i - ax_i - b)^2\n",
    "\\end{equation*}\n",
    "\n",
    "Set $\\frac{dL}{da} = 0$ and $\\frac{dL}{db} = 0$\n",
    "\n",
    "Let's do it\n",
    "\n",
    "\\begin{align*}\n",
    "&\\frac{dL}{db} = \\sum_{i=1}^{N} 2(y_i - ax_i - b)(-1) = \\sum_{i=1}^{N} (y_i - ax_i - b) = 0 \\\\\n",
    "&\\sum_{i=1}^{N} y_i- a\\sum_{i=1}^{N}x_i - bN = 0 \\\\\n",
    "& \\boxed{b = \\frac{1}{N} \\sum_{i=1}^{N} y_i - \\frac{a}{N} \\sum_{i=1}^{N}x_i}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "& \\frac{dL}{da} = \\sum_{i=1}^{N} 2(y_i - ax_i - b)(-x_i) = \\sum_{i=1}^{N} (y_i - ax_i - b)(x_i) = 0\\\\\n",
    "& \\sum_{i=1}^{N} x_iy_i - a\\sum_{i=1}^{N}x_i^2 - b\\sum_{i=1}^{N}x_i = 0 \\\\\n",
    "& \\sum_{i=1}^{N} x_iy_i = a\\sum_{i=1}^{N}x_i^2 + \\big( \\frac{1}{N} \\sum_{i=1}^{N} y_i - \\frac{a}{N} \\sum_{i=1}^{N}x_i  \\big) \\sum_{i=1}^{N}x_i \\\\\n",
    "& \\boxed{a = \\frac{\\sum_{i} x_iy_i - \\frac{1}{N}(\\sum_{i}x_i)(\\sum_{i}y_i)}{\\sum_{i}x_i^2 - \\frac{1}{N}(\\sum_{i}x_i)^2}} \n",
    "\\end{align*}\n",
    "\n",
    "So the exact solutions to the minimization problem are\n",
    "\n",
    "\\begin{align*}\n",
    "a &= \\frac{\\sum_{i} x_iy_i - \\frac{1}{N}(\\sum_{i}x_i)(\\sum_{i}y_i)}{\\sum_{i}x_i^2 - \\frac{1}{N}(\\sum_{i}x_i)^2} \\qquad \\big[= \\frac{Cov(X,Y)}{Var(X)} \\big] \\\\\n",
    "b &= \\frac{1}{N}\\sum_{i}y_i - \\frac{a}{N}\\sum_{i}x_i \\qquad[ = E[Y] - aE[X]]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "* Interpretation of $a$ and $b$\n",
    "    * $a$ – slope of the line (size of relationship between $X$ and $Y$)\n",
    "    * $b$ – intercept (the value of $\\hat{y}$ when $x=0$). Sometimes, we normalize the independent variable, $x$, by subtracting the mean, $\\bar{x}$ from all $x$. Then $b$ becomes the value of $\\hat{y}$ when $x=\\bar{x}$.\n",
    "* Interpretation of $L$, Loss function\n",
    "    * How well the model is capturing the variation in the output variable. That quantity is usually represented by $R^2$.\n",
    "    * $\\boxed{R^2 = 1 - \\frac{L}{Var(y)} = 1 - \\frac{MSE}{Var(y)} = 1 - \\frac{\\sum(y_i - \\hat{y_i})^2}{\\sum(y_i - \\bar{y})^2}}$\n",
    "    * In general, $R^2 \\in [0,1]$. The higher the $R^2$, the better the model is able to fit the observed data. Hypothetically, $R^2$ can be less than $0$, which will mean that the model is performing even worse than the simple average. In that case, we should through away that model and take the average. So in practice $R^2$ should be between $0$ and $1$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Limitations of using OLS through Anscombe’s Quartet\n",
    "* Multivariate OLS models and an example problem\n",
    "* Using matrix foundation to solve the OLS problem\n",
    "* The pros and cons of using OLS regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
