{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear and Logistic Regression, AWS Machine Learning: Data Scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression via Least Squares Minimization\n",
    "\n",
    "### Types of Machine Learning\n",
    "\n",
    "* **Supervised**. Learns a function that approximates the relationship between input and output data\n",
    "* **Unsupervised**. Finds structure in data without using explicit labels.\n",
    "    * **Classification**. Predicts discrete labels/categories. Example, Logistic Regression\n",
    "    * **Regression**. Predicts continuous values. Example, Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Linear Models\n",
    "\n",
    "* **What is it?**. In linear modeling, the relationship between each individual input variables and the output is a straight line. Slopes of such lines become the coefficients of the linear equation.\n",
    "* **An example of a linear notation**. $y_i = a_0 + a_1x_1 + a_2x_2 + ...$.\n",
    "* **Why use linear models?**.     \n",
    "    * Interpretable\n",
    "    * Low complexity\n",
    "    * Scalable\n",
    "    * Good baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of 1-Dimension Ordinary Least Squares (OLS)\n",
    "\n",
    "* $i$ - an observation. Assume $i = 1, 2, ..., N$.\n",
    "* $x$ - independent variable, feature\n",
    "* $y$ - dependent variable, output \n",
    "\n",
    "The linear relationship looks like this\n",
    "\n",
    "\\begin{equation*}\n",
    "y = ax + b\n",
    "\\end{equation*}\n",
    "\n",
    "where $a, b$ are constants.  \n",
    "\n",
    "The OLS problem is to solve for $a$ and $b$ in order to achieve $y_i = ax_i + b + \\epsilon$, where $\\epsilon$ is the noise (there is always a noise/error: no matter how well we find $a$ and $b$, the indipendent variable never fully explains the dependent variable). We usually skip $\\epsilon$ and instead write\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y}_i = ax_i + b\n",
    "\\end{equation*}\n",
    "\n",
    "where  $\\hat{y}_i$ are our estimates of $y_i$.\n",
    "\n",
    "We solve for $a$ and $b$ by defining a loss function. A loss function is used whenever we are trying to optimize a given function. Let's define a loss function for linear regression.\n",
    "\n",
    "\\begin{equation*}\n",
    "L = \\sum_{i=1}^{N}(y_i - \\hat{y_i})^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution of Ordinary Least Squares (OLS)\n",
    "\n",
    "Find the $L$ minimum by equating the first derivatives of $L$ to zero, and solving for the values of $a$ and $b$ for which $L$ is minimum.\n",
    "\n",
    "\\begin{equation*}\n",
    "L = \\sum_{i=1}^{N}(y_i - \\hat{y_i})^2 = \\sum_{i=1}^{N}(y_i - ax_i - b)^2\n",
    "\\end{equation*}\n",
    "\n",
    "Set $\\frac{dL}{da} = 0$ and $\\frac{dL}{db} = 0$\n",
    "\n",
    "Let's do it\n",
    "\n",
    "\\begin{align*}\n",
    "&\\frac{dL}{db} = \\sum_{i=1}^{N} 2(y_i - ax_i - b)(-1) = \\sum_{i=1}^{N} (y_i - ax_i - b) = 0 \\\\\n",
    "&\\sum_{i=1}^{N} y_i- a\\sum_{i=1}^{N}x_i - bN = 0 \\\\\n",
    "& \\boxed{b = \\frac{1}{N} \\sum_{i=1}^{N} y_i - \\frac{a}{N} \\sum_{i=1}^{N}x_i}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "& \\frac{dL}{da} = \\sum_{i=1}^{N} 2(y_i - ax_i - b)(-x_i) = \\sum_{i=1}^{N} (y_i - ax_i - b)(x_i) = 0\\\\\n",
    "& \\sum_{i=1}^{N} x_iy_i - a\\sum_{i=1}^{N}x_i^2 - b\\sum_{i=1}^{N}x_i = 0 \\\\\n",
    "& \\sum_{i=1}^{N} x_iy_i = a\\sum_{i=1}^{N}x_i^2 + \\big( \\frac{1}{N} \\sum_{i=1}^{N} y_i - \\frac{a}{N} \\sum_{i=1}^{N}x_i  \\big) \\sum_{i=1}^{N}x_i \\\\\n",
    "& \\boxed{a = \\frac{\\sum_{i} x_iy_i - \\frac{1}{N}(\\sum_{i}x_i)(\\sum_{i}y_i)}{\\sum_{i}x_i^2 - \\frac{1}{N}(\\sum_{i}x_i)^2}} \n",
    "\\end{align*}\n",
    "\n",
    "So the exact solutions to the minimization problem are\n",
    "\n",
    "\\begin{align*}\n",
    "a &= \\frac{\\sum_{i} x_iy_i - \\frac{1}{N}(\\sum_{i}x_i)(\\sum_{i}y_i)}{\\sum_{i}x_i^2 - \\frac{1}{N}(\\sum_{i}x_i)^2} \\qquad \\big[= \\frac{Cov(X,Y)}{Var(X)} \\big] \\\\\n",
    "b &= \\frac{1}{N}\\sum_{i}y_i - \\frac{a}{N}\\sum_{i}x_i \\qquad[ = E[Y] - aE[X]]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "* Interpretation of $a$ and $b$\n",
    "    * $a$ – slope of the line (size of relationship between $X$ and $Y$)\n",
    "    * $b$ – intercept (the value of $\\hat{y}$ when $x=0$). Sometimes, we normalize the independent variable, $x$, by subtracting the mean, $\\bar{x}$ from all $x$. Then $b$ becomes the value of $\\hat{y}$ when $x=\\bar{x}$.\n",
    "* Interpretation of $L$, Loss function\n",
    "    * How well the model is capturing the variation in the output variable. That quantity is usually represented by $R^2$.\n",
    "    * $\\boxed{R^2 = 1 - \\frac{L}{Var(y)} = 1 - \\frac{MSE}{Var(y)} = 1 - \\frac{\\sum(y_i - \\hat{y_i})^2}{\\sum(y_i - \\bar{y})^2}}$\n",
    "    * In general, $R^2 \\in [0,1]$. The higher the $R^2$, the better the model is able to fit the observed data. Hypothetically, $R^2$ can be less than $0$, which will mean that the model is performing even worse than the simple average. In that case, we should through away that model and take the average. So in practice $R^2$ should be between $0$ and $1$.\n",
    "    * **Caution**. One shouldn't use $R^2$ blindly to judge the performance of the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Ordinary Least Squares (OLS)\n",
    "\n",
    "This happens when we have more than 1 dependent variable. \n",
    "\n",
    "* $i = 1, 2, ..., N$ observations\n",
    "* $\\vec{x_i} = [x_{i1}, x_{i2}, ..., x_{iM}]$. $M$ features, independent variables\n",
    "* $y_i$ - dependent variable/output \n",
    "\n",
    "The linear relationship looks like this  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y_i} = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_M x_{iM} = \\vec{x_i} \\beta\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\beta = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ ... \\\\ \\beta_M\\end{pmatrix}$. Note, that we added $1$ to the feature vector $\\vec{x_i}$.  \n",
    "\n",
    "In matrix form, we can rewrite this model as \n",
    "\n",
    "\\begin{align*}\n",
    "& \\begin{pmatrix} y_1 \\\\ y_2 \\\\ ... \\\\ y_N\\end{pmatrix} = \\begin{pmatrix} 1 \\quad x_{1,1} \\quad x_{1,2} \\quad ... \\quad x_{1,M} \\\\ 1 \\quad  x_{2,1} \\quad x_{2,2} \\quad ... \\quad x_{2,M} \\\\ ... \\\\ 1 \\quad x_{N,1} \\quad x_{N,2} \\quad ... \\quad x_{N,M} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ ... \\\\ \\beta_M\\end{pmatrix} \\\\\n",
    "& N X 1 \\qquad\\qquad\\quad N X (M+1) \\qquad\\qquad (M+1) X 1\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\boxed{Y = X \\beta}\n",
    "\\end{equation*}\n",
    "\n",
    "The loss function is \n",
    "\n",
    "\\begin{align*}\n",
    "L &= \\sum_{i=1}^{N} (y_i - \\hat{y_i})^2 \\\\\n",
    "  &= \\sum_{i=1}^{N} (y_i - (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_M x_{iM}))^2 \\\\\n",
    "  &= \\sum_{i=1}^{N} (y_i - \\vec{x_i} \\beta)^2 \\\\\n",
    "  &= \\Vert Y - X \\beta\\Vert_2^2 \\\\\n",
    "  &= (Y -X \\beta)^T(Y - X\\beta)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\Vert . \\Vert_2$ is the $L2$ norm of the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Recall $L = (Y -X \\beta)^T(Y - X\\beta)$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{dL}{d\\beta} &= 0 \\\\\n",
    "\\frac{d(Y -X \\beta)^T(Y - X\\beta)}{\\beta} &= 0 \\\\\n",
    "\\frac{d(Y^TY - Y^TX\\beta - \\beta^TX^TY + \\beta^TX^TX\\beta}{d\\beta} &= 0 \\\\\n",
    "-(X^TY) - (X^TY) + 2 (X^TX)\\beta = 0 \\\\ \n",
    "X^TY = (X^TX)\\beta \\\\\n",
    "\\boxed{\\beta = (X^TX)^{-1}X^TY}\n",
    "\\end{align*}\n",
    "\n",
    "assuming that $X^TX$ is invertible, which happens whenever $X$ is a full rank matrix. In our case that means that $X$ should not have any columns that are linearly dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS Pros and Cons\n",
    "\n",
    "**Pros**\n",
    "* Efficient computation\n",
    "* Unique minimum\n",
    "* Stable under perturbation of data\n",
    "* Easy to interpret\n",
    "\n",
    "**Cons**\n",
    "* Influenced by outliers\n",
    "* $X^TX$ has to be invertible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression: A Probabilistic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The definition of a probabilistic approach\n",
    "* Maximum Likelihood Estimate (MLE)\n",
    "* MLE for the case of continuous probability with probability density function\n",
    "* Gaussian MLE\n",
    "* OLS as MLE\n",
    "* Reformulate OLS model\n",
    "* OLS likelihood function\n",
    "* Minimizing MSE = Maximizing MLE\n",
    "* Improvements in MLE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
